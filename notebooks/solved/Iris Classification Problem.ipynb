{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RodolfoFerro/RIIAA19-DLaaS/blob/master/notebooks/Iris%20Classification%20Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgcXbUwle_nK"
   },
   "source": [
    "> ### RIIAA 2.0 – Workshop \n",
    "> **Deep Learning as a Service** <br>\n",
    "> **Instructor:** [Rodolfo Ferro](https://rodolfoferro.xyz) <br>\n",
    "> **Email:** <ferro@cimat.mx> <br>\n",
    "> **Twitter:** <https://twitter.com/FerroRodolfo/> <br>\n",
    "> **GitHub:** <https://github.com/RodolfoFerro/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqZboFApBN18"
   },
   "source": [
    "# Iris Classification Problem\n",
    "\n",
    "Along this notebook we'll explain how to use the power of cloud computing with Google Colab for a classical example –*The Iris Classification Problem*– using the popular [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
    "\n",
    "For this classification problem we will build a simple feed-forward full-connected artificial neural network.\n",
    "\n",
    "The Python framework that we will be using is [Tensorflow 2.0](https://www.tensorflow.org) with the [Keras](https://keras.io/) module.\n",
    "\n",
    "\n",
    "### Problem statement\n",
    "\n",
    "Before we tackle the problem an ANN, let's understand what we'll be doing: \n",
    "\n",
    "* If we feed our neural network with Iris data, the model should be able to determine what species it is.\n",
    "\n",
    "> #### What do we need to do?\n",
    "> Train a _Deep Learning_ model (in this case) using a known dataset: [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
    ">\n",
    "> Specifically, we are going to do the following:\n",
    "> - Load the dataset\n",
    "> - Preprocess the data\n",
    "> - Build the model\n",
    "> - Set hyperparameters \n",
    "> - Train the model\n",
    "> - Save and download the trained model\n",
    "> - Predict data\n",
    "\n",
    "## Installing dependencies\n",
    "\n",
    "For our training we will be using Tensorflow 2.0, so we want to be sure it is installed on its latest version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7D74gMYrD0Fo"
   },
   "outputs": [],
   "source": [
    "# Let's install Tensorflow 2.0:\n",
    "!pip install -q tensorflow==2.0.0-rc0\n",
    "\n",
    "# And verify that it is now in its latest version:\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ctjdb9BD0dP"
   },
   "source": [
    "## The Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uN3Ro-5BKUq"
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "url = 'https://en.wikipedia.org/wiki/Iris_flower_data_set'\n",
    "IFrame(url, width=\"100%\", height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuiBrwaOCBbt"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDoI3H0_B7oF"
   },
   "outputs": [],
   "source": [
    "# Importing dataset from scikit-learn and other useful packages:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# We will fix a random seed for reproducibility:\n",
    "seed = 11\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtH71K6vDRff"
   },
   "outputs": [],
   "source": [
    "# We now import the Iris dataset:\n",
    "iris = load_iris()\n",
    "\n",
    "# And set the features and labels vectors from it:\n",
    "x = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "# We can load some elements to verify the contents in the dataset:\n",
    "elements_to_display = [20, 80, 120]\n",
    "for element in elements_to_display:\n",
    "    print(f\"Element {element}th:\")\n",
    "    print(f\"  - Features: {x[element]}\")\n",
    "    print(f\"  - Target: {y[element]}\")\n",
    "    print(f\"  - Species: {names[element % 3]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZMRI1V43HZg9"
   },
   "source": [
    "## Preprocess dataset\n",
    "\n",
    "The preprocess step results very important in many cases. For this case, we will just need to do a very simple transformation: a one hot encode process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2_d1fQ-HKfI"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# One hot encode outputs: \n",
    "y = keras.utils.to_categorical(y)\n",
    "\n",
    "# Set global variables:\n",
    "n_features = len(feature_names)\n",
    "n_classes = names.shape[0]\n",
    "\n",
    "# Let's checkout changes:\n",
    "for element in elements_to_display:\n",
    "    print(f\"Element {element}th:\")\n",
    "    print(f\"  - Features: {x[element]}\")\n",
    "    print(f\"  - Target: {y[element]}\")\n",
    "    print(f\"  - Species: {names[element % 3]}\")\n",
    "    print()\n",
    "\n",
    "    \n",
    "# Split the data set into training and testing sets:\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipLTk1GEIWq_"
   },
   "source": [
    "## Let's talk about the model...\n",
    "\n",
    "We will be using a very simple model, a feed-forward multi-layer perceptron.\n",
    "\n",
    "### Let's create the model with Keras!\n",
    "\n",
    "First of all, let's import what we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n42gKjsIBj8"
   },
   "outputs": [],
   "source": [
    "# Let's import our Keras stuff:\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def iris_model(input_dim, output_dim, init_nodes=4, name='model'):\n",
    "    \"\"\"FF-MLP model for Iris classification problem.\"\"\"\n",
    "    \n",
    "    # Create model:\n",
    "    model = Sequential(name=name)\n",
    "    model.add(Dense(init_nodes, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(2*init_nodes, activation='relu'))\n",
    "    model.add(Dense(3*init_nodes, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    \n",
    "    # Compile model:\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cgyhm_0dMh0R"
   },
   "source": [
    "### Useful resources\n",
    "\n",
    "- Sequential model: <https://keras.io/getting-started/sequential-model-guide/>\n",
    "- Classifying the Iris Data Set with Keras: <https://janakiev.com/notebooks/keras-iris/>\n",
    "\n",
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4ysBBcKMYhZ"
   },
   "outputs": [],
   "source": [
    "# Let's build our model:\n",
    "model = iris_model(n_features, n_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HUigT2jMyQy"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "In order to train the model, we first need to set its training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdENafOPMsbA"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "epochs = 30\n",
    "batch = 8\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(x_train, y_train, \n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=True,\n",
    "                    epochs=epochs, batch_size=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMxNyoxvOV4D"
   },
   "source": [
    "### Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLcDoeWxOU60"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model:\n",
    "scores = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(f'Test accuracy: {scores[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qByC6HBbQjiZ"
   },
   "source": [
    "### Plot the training along the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYoJzOAnQi97"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(\"Model's training loss\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(\"Model's training accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kdabtBjNKwK"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHYbaMS2UhVZ"
   },
   "source": [
    "_How can we save these plots?_\n",
    "\n",
    "## Saving a model\n",
    "\n",
    "To save the trained model we will basically do two things:\n",
    "\n",
    "1. Serialize the model into a JSON file, which will save the architecture of our model.\n",
    "2. Serialize the weights into a HDF5 file, which will save all parameters of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kq7y3CtkQnTT"
   },
   "outputs": [],
   "source": [
    "# Serialize model to JSON:\n",
    "model_json = model.to_json()\n",
    "with open(\"iris_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Serialize weights to HDF5 (h5py needed):\n",
    "model.save_weights(\"iris_model.h5\")\n",
    "print(\"Model saved to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJ1OGbhTVo82"
   },
   "source": [
    "## Downloading a model\n",
    "\n",
    "We just need to import the Google Colab module and download the specified files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZ9Z34ndVwvv"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "model_files = ['iris_model.json', 'iris_model.h5']\n",
    "for file in model_files:\n",
    "    files.download(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XzHznMlxXgry"
   },
   "source": [
    "## Loading a trained model\n",
    "We will basically do three things:\n",
    "\n",
    "1. Load the model from a JSON file.\n",
    "2. Load the weights from a HDF5 file.\n",
    "3. (Re)Compile the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZtlTbNpWdAM"
   },
   "outputs": [],
   "source": [
    "# Load json and create model:\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "json_file = open('iris_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load weights into loaded model:\n",
    "loaded_model.load_weights(\"iris_model.h5\")\n",
    "print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qnt7JaIGXxk8"
   },
   "outputs": [],
   "source": [
    "# Evaluate loaded model on test data:\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f'Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdg_v2slX9Ou"
   },
   "source": [
    "## Predicting from new data\n",
    "\n",
    "Now that we have a trained model, how do we use it?\n",
    "\n",
    "It is as simple as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7fUV0QyX5sB"
   },
   "outputs": [],
   "source": [
    "# Remembering some elements:\n",
    "for element in elements_to_display:\n",
    "    prediction_vector = model.predict(np.array([x[element]]))\n",
    "    print(f\"Element {element}th:\")\n",
    "    print(f\"  - Features: {x[element]}\")\n",
    "    print(f\"  - Target: {y[element]}\")\n",
    "    print(f\"  - Scpecies: {names[np.argmax(y[element])]}\")\n",
    "    print(f\"  - Predicted species: {names[np.argmax(prediction_vector)]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVA5PIfXYWc2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Iris Classification Problem",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
