{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Biomedical Images Segmentation",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/RIIAA19-DLaaS/blob/master/notebooks/Biomedical%20Images%20Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgcXbUwle_nK",
        "colab_type": "text"
      },
      "source": [
        "> ### RIIAA 2.0 – Workshop \n",
        "> **Deep Learning as a Service** <br>\n",
        "> **Instructor:** [Rodolfo Ferro](https://rodolfoferro.xyz) <br>\n",
        "> **Email:** <ferro@cimat.mx> <br>\n",
        "> **Twitter:** <https://twitter.com/FerroRodolfo/> <br>\n",
        "> **GitHub:** <https://github.com/RodolfoFerro/> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqZboFApBN18",
        "colab_type": "text"
      },
      "source": [
        "# Biomedical Images Segmentation\n",
        "\n",
        "Along this notebook we'll explain how to use the power of cloud computing with Google Colab for a non-so-classical example, we are going to do biomedical image segmentation based on the [ISBI Challenge](http://brainiac2.mit.edu/isbi_challenge/).\n",
        "\n",
        "For this classification problem we will build a nice and elaborated convolutional neural network, based on **U-Net: Convolutional Networks for Biomedical Image Segmentation** ([*Ronneberger et. al*](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)).\n",
        "\n",
        "The Python framework that we will be using is [Tensorflow 2.0](https://www.tensorflow.org) with the [Keras](https://keras.io/) module.\n",
        "\n",
        "\n",
        "### Problem statement\n",
        "\n",
        "Before we tackle the problem an elegant CNN, let's understand what we'll be doing: \n",
        "\n",
        "* If we feed our neural network with raw biomedical data, the model should be able to create a segmentation map for the input image.\n",
        "\n",
        "> #### What do we need to do?\n",
        "> Train a _Deep Learning_ model (in this case) using a dataset from a challenge: [ISBI Challenge](http://brainiac2.mit.edu/isbi_challenge/).\n",
        ">\n",
        "> Specifically, we are going to do the following:\n",
        "> - Load the dataset\n",
        "> - Preprocess the data\n",
        "> - Build the model\n",
        "> - Set hyperparameters \n",
        "> - Train the model\n",
        "> - Save and download the trained model\n",
        "> - Predict data\n",
        "\n",
        "## Installing dependencies\n",
        "\n",
        "For our training we will be using Tensorflow 2.0, so we want to be sure it is installed on its latest version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D74gMYrD0Fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's install Tensorflow 2.0:\n",
        "!pip install -q tensorflow==2.0.0-rc0\n",
        "\n",
        "# And verify that it is now in its latest version:\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ctjdb9BD0dP",
        "colab_type": "text"
      },
      "source": [
        "## The membrane dataset\n",
        "\n",
        "About the dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uN3Ro-5BKUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/RodolfoFerro/RIIAA19-DLaaS/raw/master/data/membrane.zip\n",
        "!unzip membrane.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhK3yHcDQFtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls membrane\n",
        "!ls membrane/test\n",
        "!ls membrane/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj3iTV6RQnFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "\n",
        "training_path_img = sorted(glob(\"/content/membrane/train/image/*\"))\n",
        "training_path_lbl = sorted(glob(\"/content/membrane/train/label/*\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuiBrwaOCBbt",
        "colab_type": "text"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDoI3H0_B7oF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the dataset with OpenCV and other useful packages:\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# We will fix a random seed for reproducibility:\n",
        "seed = 11\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Loading function:\n",
        "def image_loader(images_list):\n",
        "    \"\"\"Function to load images into arrays from folder path.\"\"\"\n",
        "\n",
        "    images = []\n",
        "    for image in images_list:\n",
        "        img = cv2.imread(image)\n",
        "        images.append(img[:,:,0])\n",
        "    \n",
        "    return np.array(images)\n",
        "\n",
        "# Data processor:\n",
        "def image_generator(images, fraction=2):\n",
        "    def image_cropper(image, fraction):\n",
        "        w, h = image.shape[:2]\n",
        "        wbase, hbase = w//fraction, h//fraction\n",
        "        cropped_images = []\n",
        "        for i in range(fraction):\n",
        "            for j in range(fraction):\n",
        "                a = i*hbase\n",
        "                b = i*hbase + hbase\n",
        "                c = j*wbase\n",
        "                d = j*wbase + wbase\n",
        "                img = image[a:b, c:d]\n",
        "                cropped_images.append(img)\n",
        "        return np.array(cropped_images)\n",
        "    \n",
        "    cropped_images = []\n",
        "    for image in images:\n",
        "        for img in image_cropper(image, fraction):\n",
        "            cropped_images.append(img)\n",
        "    \n",
        "    return np.array(cropped_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtH71K6vDRff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We now import the Iris dataset:\n",
        "x_train = image_loader(training_path_img)\n",
        "y_train = image_loader(training_path_lbl)\n",
        "\n",
        "# Display first element from (x_train, y_train):\n",
        "plt.figure()\n",
        "plt.title(\"First element from training dataset:\")\n",
        "plt.imshow(x_train[0], cmap=\"gray\")\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"First outcome from training dataset:\")\n",
        "plt.imshow(y_train[0], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMRI1V43HZg9",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess dataset\n",
        "\n",
        "The preprocess step results very important in many cases. For this case, we will just need to do a very simple transformation: a one hot encode process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB9RbbxBh-fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = image_generator(x_train, fraction=16)\n",
        "y_train = image_generator(y_train, fraction=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TELMOS5Vj2AQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_train[0].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2_d1fQ-HKfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape to be [samples][pixels][width][height]:\n",
        "x_train = x_train.reshape(x_train.shape[0], 32, 32, 1).astype('float32')\n",
        "y_train = y_train.reshape(y_train.shape[0], 32, 32, 1).astype('float32')\n",
        "\n",
        "# Normalize inputs from 0-255 to 0-1:\n",
        "x_train /= 255\n",
        "y_train /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipLTk1GEIWq_",
        "colab_type": "text"
      },
      "source": [
        "## Let's talk about the model...\n",
        "\n",
        "We will be using a very simple model, a feed-forward multi-layer perceptron.\n",
        "\n",
        "### Let's create the model with Keras!\n",
        "\n",
        "First of all, let's import what we'll use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n42gKjsIBj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's import our Keras stuff:\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "\n",
        "def unet_based_model(input_dim, name='model'):\n",
        "    \"\"\"U-Net based model for biomedical segmentation.\"\"\"\n",
        "    \n",
        "    # Create model:\n",
        "    inputs = Input(input_dim)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv10)\n",
        "    \n",
        "    # Compile model:\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgyhm_0dMh0R",
        "colab_type": "text"
      },
      "source": [
        "### Useful resources\n",
        "\n",
        "- Sequential model: <https://keras.io/getting-started/sequential-model-guide/>\n",
        "- Classifying the Iris Data Set with Keras: <https://janakiev.com/notebooks/keras-iris/>\n",
        "\n",
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4ysBBcKMYhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's build our model:\n",
        "model = unet_based_model(input_dim=(32, 32, 1), name='UNet_model')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HUigT2jMyQy",
        "colab_type": "text"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "In order to train the model, we first need to set its training hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdENafOPMsbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set hyperparameters\n",
        "epochs = 5\n",
        "batch = 8\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(x_train, y_train, \n",
        "                    verbose=True,\n",
        "                    epochs=epochs, batch_size=batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qByC6HBbQjiZ",
        "colab_type": "text"
      },
      "source": [
        "### Plot the training along the time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYoJzOAnQi97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(history):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.title(\"Model's training loss\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['Train'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_accuracy(history):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.title(\"Model's training accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(['Train'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kdabtBjNKwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss(history)\n",
        "plot_accuracy(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHYbaMS2UhVZ",
        "colab_type": "text"
      },
      "source": [
        "_How can we save these plots?_\n",
        "\n",
        "## Saving a model\n",
        "\n",
        "To save the trained model we will basically do two things:\n",
        "\n",
        "1. Serialize the model into a JSON file, which will save the architecture of our model.\n",
        "2. Serialize the weights into a HDF5 file, which will save all parameters of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq7y3CtkQnTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Serialize model to JSON:\n",
        "model_json = model.to_json()\n",
        "with open(\"segmentation_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Serialize weights to HDF5 (h5py needed):\n",
        "model.save_weights(\"segmentation_model.h5\")\n",
        "print(\"Model saved to disk.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ1OGbhTVo82",
        "colab_type": "text"
      },
      "source": [
        "## Downloading a model\n",
        "\n",
        "We just need to import the Google Colab module and download the specified files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ9Z34ndVwvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "model_files = ['segmentation_model.json', 'segmentation_model.h5']\n",
        "for file in model_files:\n",
        "    files.download(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzHznMlxXgry",
        "colab_type": "text"
      },
      "source": [
        "## Loading a trained model\n",
        "We will basically do three things:\n",
        "\n",
        "1. Load the model from a JSON file.\n",
        "2. Load the weights from a HDF5 file.\n",
        "3. (Re)Compile the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZtlTbNpWdAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load json and create model:\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "json_file = open('segmentation_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# Load weights into loaded model:\n",
        "loaded_model.load_weights(\"segmentation_model.h5\")\n",
        "print(\"Model loaded from disk.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnt7JaIGXxk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate loaded model on test data:\n",
        "loaded_model.compile(loss='binary_crossentropy',\n",
        "                     optimizer='adam',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "score = loaded_model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdg_v2slX9Ou",
        "colab_type": "text"
      },
      "source": [
        "## Predicting from new data\n",
        "\n",
        "Now that we have a trained model, how do we use it?\n",
        "\n",
        "It is as simple as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7fUV0QyX5sB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's predict from out testing folder:\n",
        "testing_path = sorted(glob(\"/content/membrane/test/*\"))\n",
        "\n",
        "# Load test image for prediction:\n",
        "img_index = 5\n",
        "img = cv2.imread(testing_path[img_index])\n",
        "img = img[:, :, 0]\n",
        "\n",
        "# Transform input image for testing:\n",
        "test_img = cv2.resize(img, (32, 32))\n",
        "test_img = test_img.reshape(1, 32, 32, 1).astype('float32')\n",
        "print(test_img.shape)\n",
        "\n",
        "# Predict and transform output image:\n",
        "prediction = model.predict(test_img)\n",
        "prediction = prediction.reshape(32, 32).astype('float32')\n",
        "#prediction = cv2.resize(prediction, (512, 512))\n",
        "print(prediction.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVA5PIfXYWc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display first element from (x_train, y_train):\n",
        "plt.figure()\n",
        "plt.title(\"Original image:\")\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.grid(False)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Predicted output:\")\n",
        "plt.imshow(prediction, cmap=\"gray\")\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzKYKa-Ix1jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}